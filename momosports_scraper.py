# æ–‡ä»¶å: momosports_scraper.py

import json
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from core_scraper import CoreScraper, requests
import time
import re

class MomoSportsScraper(CoreScraper):
    """
    Momo Sports ä¸“å±çˆ¬è™«ï¼Œå®ç°äº†å…ˆæŠ“HTMLé¦–é¡µï¼Œå†æŠ“APIåç»­é¡µçš„æ··åˆé€»è¾‘ã€‚
    """

    def _get_total_count(self, soup):
        """ä»HTMLæˆ–HTMLç‰‡æ®µä¸­æå–å•†å“æ€»æ•°ã€‚"""
        try:
            toolbar_amount = soup.select_one("p.toolbar-amount")
            if toolbar_amount:
                # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… "of 157" è¿™æ ·çš„æ¨¡å¼
                match = re.search(r'of\s+(\d+)', toolbar_amount.text)
                if match:
                    return int(match.group(1))
        except Exception:
            return 0
        return 0

    def fetch_data(self):
        """å®ç°ä¸¤æ­¥èµ°ç­–ç•¥ï¼šå…ˆæŠ“HTMLï¼Œå†æŠ“APIã€‚"""
        all_products = []
        pagination = self.cfg.get("pagination", {})
        page_size = pagination.get("page_size", 36)
        
        session = requests.Session()
        session.headers.update(self.headers)
        
        # --- ç¬¬1æ­¥: æŠ“å–å¹¶è§£æç¬¬ä¸€é¡µ (HTML) ---
        try:
            main_page_url = f"{self.api_url}?product_list_limit={page_size}"
            self.log(f"ğŸ“¦ æ­£åœ¨æŠ“å–ç¬¬ 1 é¡µ (é€šè¿‡è§£æHTML)...")
            response = session.get(main_page_url, impersonate=self.impersonate, timeout=30)
            response.raise_for_status()
            
            # Sessionä¼šè‡ªåŠ¨ä¿å­˜Cookieï¼ŒåŒæ—¶æˆ‘ä»¬ç›´æ¥è§£æè¿™ä¸ªé¡µé¢çš„HTML
            page1_products = self.parse_data(response.text, self.base_url)
            all_products.extend(page1_products)
            self.log(f"âœ… ç¬¬ 1 é¡µè§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(page1_products)} ä¸ªå•†å“ã€‚")
            
            # ä»ç¬¬ä¸€é¡µè·å–å•†å“æ€»æ•°ï¼Œä»¥å†³å®šæ€»å…±éœ€è¦ç¿»å¤šå°‘é¡µ
            soup = BeautifulSoup(response.text, 'lxml')
            total_count = self._get_total_count(soup)
            if total_count > 0:
                total_pages = (total_count + page_size - 1) // page_size
                self.log(f"â„¹ï¸ å•†å“æ€»æ•°: {total_count}ï¼Œå…±è®¡ {total_pages} é¡µã€‚")
            else:
                total_pages = pagination.get("max_pages", 10)
                self.log(f"âš ï¸ æœªèƒ½è·å–å•†å“æ€»æ•°ï¼Œå°†æŒ‰æœ€å¤§é¡µæ•° {total_pages} æŠ“å–ã€‚")

        except Exception as e:
            self.log(f"âŒ æŠ“å–ç¬¬ 1 é¡µ (HTML) å¤±è´¥: {e}")
            return [] # å¦‚æœç¬¬ä¸€é¡µéƒ½å¤±è´¥äº†ï¼Œå°±æ²¡å¿…è¦ç»§ç»­äº†

        # --- ç¬¬2æ­¥: å¾ªç¯æŠ“å–åç»­é¡µ (API) ---
        for page in range(2, total_pages + 1):
            self.log(f"ğŸ“¦ æ­£åœ¨æŠ“å–ç¬¬ {page}/{total_pages} é¡µ (é€šè¿‡API)...")
            params = {
                'p': page,
                'product_list_limit': page_size,
                'shopbyAjax': 1
            }
            try:
                response = session.get(self.api_url, params=params, impersonate=self.impersonate, timeout=20)
                response.raise_for_status()
                json_data = response.json()
                html_content = json_data.get('categoryProducts')

                if not html_content:
                    self.log("â„¹ï¸ APIæœªè¿”å›å•†å“HTMLå†…å®¹ï¼Œåœæ­¢ç¿»é¡µã€‚")
                    break

                page_products = self.parse_data(html_content, self.base_url)
                if not page_products: break
                
                all_products.extend(page_products)
                time.sleep(self.cfg.get("delay", 1))
            except Exception as e:
                self.log(f"âŒ æŠ“å–ç¬¬ {page} é¡µ (API) å¤±è´¥: {e}")
                break
        return all_products

    def parse_data(self, html_text, base_url):
        """è§£æHTMLç‰‡æ®µï¼Œæ­¤æ–¹æ³•è¢«ä¸¤æ­¥ç­–ç•¥å…±ç”¨ã€‚"""
        self.log("ğŸ¤– æ­£åœ¨ä½¿ç”¨ BeautifulSoup è§£æHTMLå†…å®¹...")
        try:
            soup = BeautifulSoup(html_text, 'lxml')
            products = []
            product_items = soup.select('li.product-item')
            for item in product_items:
                try:
                    name_tag = item.select_one('a.product-item-link')
                    link_tag = item.select_one('a.product-item-photo')
                    image_tag = item.select_one('img.product-image-photo')
                    sku_id_raw = item.get('id', '')
                    final_price_tag = item.select_one('.price-final_price .price')
                    old_price_tag = item.select_one('.old-price .price')

                    if not (name_tag and link_tag and sku_id_raw and final_price_tag): continue
                    
                    name = name_tag.text.strip()
                    product_url = link_tag.get('href')
                    image_url = image_tag.get('src') if image_tag else None
                    sku_id = sku_id_raw.replace('product-sku-', '')
                    sale_price = float(re.sub(r'[^\d.]', '', final_price_tag.text))
                    list_price = float(re.sub(r'[^\d.]', '', old_price_tag.text)) if old_price_tag else sale_price
                    discount = round((1 - sale_price / list_price) * 100) if list_price > sale_price else 0

                    products.append({
                        "sku_id": sku_id, "product_id": sku_id, "name": name, "url": product_url, 
                        "image_url": image_url, "list_price": list_price, "sale_price": sale_price, 
                        "discount_percentage": discount, "color": None, "size": None
                    })
                except Exception as e:
                    self.log(f"âš ï¸ è§£æå•ä¸ªå•†å“æ—¶å‡ºé”™: {e}")
            self.log(f"âœ… è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(products)} ä¸ªå•†å“ã€‚")
            return products
        except Exception as e:
            self.log(f"âŒ åœ¨MomoSportsScraperä¸­è§£æHTMLæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            return []
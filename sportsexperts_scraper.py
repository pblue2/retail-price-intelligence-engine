# æ–‡ä»¶å: sportsexperts_scraper.py

from urllib.parse import urljoin
from bs4 import BeautifulSoup
from core_scraper import CoreScraper, requests
import time

class SportsExpertsScraper(CoreScraper):
    """
    Sports Experts ä¸“å±çˆ¬è™«ç±»ï¼Œå®ç°äº†ä¼šè¯ç®¡ç†å’Œæ­£ç¡®çš„JSONè§£æé€»è¾‘ã€‚
    """
    
    def _parse_html_products(self, html_text, base_url):
        soup = BeautifulSoup(html_text, 'lxml')
        products = []
        product_tiles = soup.select('div.product-tile[data-product-id]')
        for tile in product_tiles:
            try:
                product_id = tile.get('data-product-id')
                name_tag = tile.select_one('a[data-qa="search-product-title"]')
                price_tag = tile.select_one('span[data-qa="search-product-price"]')
                link_tag = tile.select_one('a.product-tile-media')
                image_tag = tile.select_one('img.img-fluid')
                if not (product_id and name_tag and price_tag and link_tag): continue
                name = name_tag.text.strip()
                price_str = price_tag.text.strip().replace('$', '').replace(',', '')
                price = float(price_str) if price_str else 0.0
                product_url = urljoin(base_url, link_tag.get('href'))
                image_url = image_tag.get('src') if image_tag else None
                href = link_tag.get('href', '')
                sku_id = href.split('/')[-1] if '/' in href else product_id
                products.append({
                    "sku_id": sku_id, "product_id": product_id, "name": name, "url": product_url, 
                    "image_url": image_url, "list_price": price, "sale_price": price, 
                    "discount_percentage": 0, "color": None, "size": None
                })
            except Exception as e: self.log(f"âš ï¸ è§£æå•ä¸ªHTMLå•†å“æ—¶å‡ºé”™: {e}")
        return products

    def _parse_json_products(self, data, base_url):
        """
        ä¸“é—¨ç”¨äºè§£æåç»­é¡µé¢çš„API JSONï¼Œç§»é™¤äº†é”™è¯¯çš„å»é‡é€»è¾‘ã€‚
        """
        search_results = data.get("ProductSearchResults", {})
        items = search_results.get("SearchResults", [])
        total_count = search_results.get("TotalCount", 0)
        products = []
        
        # --- æ ¸å¿ƒä¿®æ­£ï¼šä¸å†æŒ‰ ProductId å»é‡ ---
        for item in items:
            product_id = item.get("ProductId")
            pricing = item.get("Pricing", {})
            list_price = pricing.get("ListPrice")
            sale_price = pricing.get("Price") or list_price
            discount = round((1 - sale_price / list_price) * 100) if list_price and sale_price and list_price > sale_price else 0
            
            products.append({
                "sku_id": item.get("VariantId"), # æ¯ä¸ªVariantéƒ½æ˜¯å”¯ä¸€çš„
                "product_id": product_id,
                "name": item.get("DisplayName"),
                "url": urljoin(base_url, item.get("Url")),
                "image_url": item.get("ImageUrl"),
                "list_price": list_price,
                "sale_price": sale_price,
                "discount_percentage": discount,
                "color": None, "size": None
            })
        return products, total_count


    def fetch_data(self):
        """é‡å†™æ•°æ®æŠ“å–æ–¹æ³•ï¼Œå¼•å…¥Sessionå¯¹è±¡æ¥è‡ªåŠ¨ç®¡ç†Cookieã€‚"""
        all_products = []
        
        session = requests.Session()
        session.headers.update(self.headers)
        
        try:
            main_page_url = self.cfg.get("main_page_url")
            if not main_page_url:
                self.log("âŒ é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ 'main_page_url'ã€‚")
                return []
            self.log(f"ğŸ“¦ æ­£åœ¨è®¿é—®ä¸»é¡µä»¥è·å–Cookie...")
            response = session.get(main_page_url, impersonate=self.impersonate, timeout=30)
            response.raise_for_status()
            
            self.log(f"âœ… ç¬¬ 1 é¡µHTMLå†…å®¹è·å–æˆåŠŸï¼Œå¼€å§‹è§£æ...")
            page1_products = self._parse_html_products(response.text, self.base_url)
            all_products.extend(page1_products)
            self.log(f"âœ… ç¬¬ 1 é¡µè§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(page1_products)} ä¸ªå•†å“ã€‚")
        except Exception as e:
            self.log(f"âŒ æŠ“å–ç¬¬ 1 é¡µ (HTML) å¤±è´¥: {e}")
        
        pagination = self.cfg.get("pagination", {})
        page_size = pagination.get("page_size", 24)
        max_pages = pagination.get("max_pages", 15)
        
        # ä½¿ç”¨ä¸€ä¸ªå˜é‡æ¥å­˜å‚¨ä»APIè·å–çš„å•†å“æ€»æ•°
        total_api_count = 0

        for page in range(2, max_pages + 1):
            self.log(f"ğŸ“¦ æ­£åœ¨æŠ“å–ç¬¬ {page}/{max_pages} é¡µ (é€šè¿‡API)...")
            payload = self.payload_template.copy()
            payload["Page"] = page
            payload["StartIndex"] = (page - 1) * page_size
            
            try:
                response = session.post(self.api_url, json=payload, impersonate=self.impersonate, timeout=20)
                response.raise_for_status()
                json_data = response.json()
                
                page_products, total_api_count = self._parse_json_products(json_data, self.base_url)
                
                if not page_products:
                    self.log("â„¹ï¸ APIè¿”å›å†…å®¹ä¸ºç©ºï¼Œå·²æŠ“å–å®Œæ‰€æœ‰åç»­é¡µé¢ï¼Œåœæ­¢ç¿»é¡µã€‚")
                    break
                all_products.extend(page_products)

                # ä½¿ç”¨ä»APIè·å–çš„æ€»æ•°æ¥åˆ¤æ–­æ˜¯å¦æå‰ç»“æŸ
                if total_api_count > 0 and len(all_products) >= total_api_count:
                    self.log(f"å·²æŠ“å– {len(all_products)}/{total_api_count} ä¸ªå•†å“ï¼Œæå‰ç»“æŸã€‚")
                    break
                
                time.sleep(self.cfg.get("delay", 1))
            except Exception as e:
                self.log(f"âŒ æŠ“å–ç¬¬ {page} é¡µ (API) å¤±è´¥: {e}")
                break
        return all_products